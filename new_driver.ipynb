{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 driver script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw1 import Critic, Composer\n",
    "from midi2seq import process_midi_seq, seq2piano, random_piano, piano2seq, segment\n",
    "\n",
    "import torch # Requried to create tensors to store all numerical values\n",
    "import torch.nn as nn # Required for weight and bias tensors\n",
    "import torch.nn.functional as F # Required for the activation functions\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset # How do we use them?\n",
    "\n",
    "import numpy as np\n",
    "root = './'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 50\n",
    "seq_to_gen = 20000\n",
    "seq_len = 51 # This will be the default used by Professor\n",
    "epoch = 500\n",
    "loss = None\n",
    "\n",
    "conv_val = 0.001\n",
    "conv_flag = False\n",
    "\n",
    "#* Part of this is adopted from https://github.com/alizano94/CSC-7343-HW1/blob/main/HW1/main.py\n",
    "midi_load = process_midi_seq(datadir=root, n=seq_to_gen, maxlen=(seq_len - 1))\n",
    "loader_composer = DataLoader(TensorDataset(torch.from_numpy(midi_load)), shuffle=True, batch_size=batch_size, num_workers=4) # Composer training dataset\n",
    "\n",
    "\n",
    "cps = Composer(seq_len = (seq_len - 1)) # Loads composer model with default values\n",
    "cps.model.to(device=device)\n",
    "for i in range(epoch):\n",
    "    for idx, x in enumerate(loader_composer):\n",
    "        loss = cps.train(x[0].cuda(0).long())\n",
    "        print(f\"Epoch: {i}, Batch: {idx}, Loss: {loss.item()}\")\n",
    "        if (loss.item() < conv_val):\n",
    "            print(f\"convergence reached\")\n",
    "            conv_flag = True\n",
    "        \n",
    "        if(conv_flag):\n",
    "            break # \n",
    "    \n",
    "    if(conv_flag):\n",
    "            print(f\"Training complete with loss: {loss.item()}\")\n",
    "            break # \n",
    "\n",
    "# Save model\n",
    "torch.save(cps.model.state_dict(), \"az_cps.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test music composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Works do not delete\n",
    "# for i in range(epochs):\n",
    "#     for idx, batch in enumerate(critic_train_loader):\n",
    "#         loss_val = critic_model.train(batch)\n",
    "#         print(f\"Epoch: {i}, Batch: {idx}, Loss: {loss_val.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train critic\n",
    "\n",
    "#* Part of this is adopted from https://github.com/alizano94/CSC-7343-HW1/blob/main/HW1/main.py\n",
    "# midi_load = process_midi_seq(datadir=root, n=seq_to_gen, maxlen=(seq_len - 1))\n",
    "# lbs = np.ones((midi_load.shape[0], 1))\n",
    "\n",
    "# train_size = int(0.7*midi_load.shape[0])\n",
    "# train_seq_data = midi_load[0:int(train_size),:]\n",
    "# test_seq_data = midi_load[int(train_size):,:]\n",
    "\n",
    "# train_labels = np.ones(train_seq_data.shape[0])\n",
    "# test_labels = np.ones(test_seq_data.shape[0])\n",
    "\n",
    "# # Create bad samples for training\n",
    "# for i in range(train_seq_data.shape[0]):\n",
    "#     rand_midi = random_piano(n = 250)\n",
    "#     rand_seq = piano2seq(rand_midi)\n",
    "#     rand_seq =  rand_seq[0:seq_len]\n",
    "#     train_seq_data = np.vstack((train_seq_data, rand_seq))\n",
    "#     train_labels = np.append(train_labels,0)\n",
    "\n",
    "# # Create bad samples for testing\n",
    "# for i in range(test_seq_data.shape[0]):\n",
    "#     rand_midi = random_piano(n = 250)\n",
    "#     rand_seq = piano2seq(rand_midi)\n",
    "#     rand_seq =  rand_seq[0:seq_len]\n",
    "#     test_seq_data = np.vstack((test_seq_data, rand_seq))\n",
    "#     test_labels = np.append(test_labels,0)\n",
    "\n",
    "# x_train_tensors = torch.Tensor(train_seq_data)\n",
    "# y_train_tensors = torch.Tensor(train_labels)\n",
    "\n",
    "\n",
    "# x_train_tensors_final = torch.reshape(x_train_tensors,\n",
    "#                                     (x_train_tensors.shape[0],\n",
    "#                                     1, x_train_tensors.shape[1]))\n",
    "\n",
    "# print(x_train_tensors_final.shape)\n",
    "# # print(x_train_tensors_final[0])\n",
    "# print(y_train_tensors.shape)\n",
    "\n",
    "# loader_critic = DataLoader(TensorDataset(x_train_tensors, y_train_tensors), batch_size=batch_size, shuffle=True) # Critic training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose music\n",
    "# Critic music"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc_7343",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
